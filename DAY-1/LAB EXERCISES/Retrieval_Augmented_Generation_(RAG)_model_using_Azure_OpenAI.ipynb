{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pm1GUvZQiYp2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0e4f891-a83c-40fe-99d8-1a29304df385"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.10.5)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.12.0\n"
          ]
        }
      ],
      "source": [
        "!pip install requests faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Sample documents (small knowledge base)\n",
        "documents = [\n",
        "    \"Python is a programming language.\",\n",
        "    \"The capital of France is Paris.\",\n",
        "    \"Azure is a cloud computing platform by Microsoft.\",\n",
        "    \"OpenAI provides powerful language models for various tasks.\",\n",
        "    \"The Great Wall of China is a historic landmark.\"\n",
        "]\n",
        "\n",
        "# Convert documents into TF-IDF vectors\n",
        "vectorizer = TfidfVectorizer()\n",
        "document_vectors = vectorizer.fit_transform(documents).toarray()\n",
        "\n",
        "# Index the document vectors using FAISS (for fast retrieval)\n",
        "index = faiss.IndexFlatL2(document_vectors.shape[1])\n",
        "index.add(np.array(document_vectors).astype(np.float32))\n",
        "\n",
        "# Function to retrieve the most relevant document\n",
        "def retrieve(query, top_n=1):\n",
        "    query_vector = vectorizer.transform([query]).toarray()\n",
        "    distances, indices = index.search(query_vector.astype(np.float32), top_n)\n",
        "    return [documents[i] for i in indices[0]]\n"
      ],
      "metadata": {
        "id": "igBrS0YAjF4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "\n",
        "# Set up Azure OpenAI credentials\n",
        "api_key = \"FPMNN73KO2lW7hWmdTT5DlpZY8YSeC88VQIqmSk5xDy5JDYWpt4wJQQJ99BKACYeBjFXJ3w3AAAAACOGs5Iw\"\n",
        "endpoint = \"https://hks-demo-new.cognitiveservices.azure.com/\"\n",
        "deployment_name = \"gpt-35-turbo\"  # Replace with your deployment name\n",
        "\n",
        "# Create the full endpoint URL\n",
        "url = f\"{endpoint}openai/deployments/{deployment_name}/chat/completions?api-version=2024-10-01-preview\"\n",
        "\n",
        "\n",
        "# Set up headers\n",
        "headers = {\n",
        "    \"Content-Type\": \"application/json\",\n",
        "    \"api-key\": api_key\n",
        "}\n"
      ],
      "metadata": {
        "id": "oT_X1JeqjJ3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rag_pipeline(query):\n",
        "    # Step 1: Retrieve the most relevant document\n",
        "    retrieved_docs = retrieve(query, top_n=1)\n",
        "    context = \" \".join(retrieved_docs)\n",
        "    print(\"Retrieved Document:\", context)\n",
        "\n",
        "    # Step 2: Use Azure OpenAI to generate an answer using the retrieved document\n",
        "    prompt = f\"Based on the following context, answer the question: {query}\\n\\nContext: {context}\"\n",
        "\n",
        "    # Create the payload\n",
        "    payload = {\n",
        "      \"messages\":[{\n",
        "          \"role\":\"system\",\n",
        "          \"content\":\"You are an AI assistant that helps people find information.  Only provide answer based on the context that is provided\"\n",
        "        },{\n",
        "          \"role\":\"user\",\n",
        "          \"content\":prompt\n",
        "      }],\n",
        "      \"max_tokens\": 100,\n",
        "      \"temperature\": 0.7\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "    # Send the request to Azure OpenAI API\n",
        "    print(f\"Sending request to {url}\")\n",
        "    response = requests.post(url, headers=headers, json=payload)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        result = response.json()\n",
        "        # return result['choices'][0]['text'].strip()\n",
        "        return result['choices'][0]['message']['content'].strip()\n",
        "    else:\n",
        "        return f\"Error: {response.status_code}, {response.text}\"\n"
      ],
      "metadata": {
        "id": "5R9lfcxnjUb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = rag_pipeline(\"What is the capital of France?\")\n",
        "print(\"Final Response:\", response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrxFJjIWjbPK",
        "outputId": "0192f70c-964b-4dc9-eb74-62d380997f3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieved Document: The capital of France is Paris.\n",
            "Sending request to https://hks-demo-new.cognitiveservices.azure.com/openai/deployments/gpt-35-turbo/chat/completions?api-version=2024-10-01-preview\n",
            "Final Response: The capital of France is Paris.\n"
          ]
        }
      ]
    }
  ]
}